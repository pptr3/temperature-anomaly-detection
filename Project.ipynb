{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "import matplotlib\n",
    "import seaborn\n",
    "import matplotlib.dates as md\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "#from pyemma import msm # not available on Kaggle Kernel\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd445f0cbf964528aa16901201d9ae46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'data/ambient_temperature_system_failure.csv'\n",
    "df = pd.read_csv(file_name)\n",
    "\n",
    "def plot_series(df):\n",
    "    # change the type of timestamp column for plotting\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    # change fahrenheit to °C (temperature mean= 71 -> fahrenheit)\n",
    "    df['value'] = (df['value'] - 32) * 5/9\n",
    "    # plot the data\n",
    "    df.plot(x='timestamp', y='value')\n",
    "    \n",
    "plot_series(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the hours and if it's night or day (7:00-22:00)\n",
    "df['hours'] = df['timestamp'].dt.hour\n",
    "df['daylight'] = ((df['hours'] >= 7) & (df['hours'] <= 22)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DayOfTheWeek'] = df['timestamp'].dt.dayofweek\n",
    "df['WeekDay'] = (df['DayOfTheWeek'] < 5).astype(int)\n",
    "# An estimation of anomly population of the dataset (necessary for several algorithm)\n",
    "outliers_fraction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time with int to plot easily\n",
    "df['time_epoch'] = (df['timestamp'].astype(np.int64)/100000000000).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of 4 distinct categories that seem useful: weekend night, weekend light, weekday night, weekday light\n",
    "df['categories'] = df['WeekDay']*2 + df['daylight']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "### Cluster only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take useful feature and standardize them\n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# reduce to 2 importants features\n",
    "pca = PCA(n_components=2)\n",
    "data = pca.fit_transform(data)\n",
    "# standardize these 2 new features\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method kmens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f89723a5930492a8b0e790d789443e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate with different number of centroids to see the loss plot (elbow method)\n",
    "n_cluster = range(1, 20)\n",
    "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
    "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(n_cluster, scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3721\n",
       "1    1522\n",
       "2    1428\n",
       "3     596\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I choose 4 centroids arbitrarily and add these data to the central dataframe\n",
    "df['cluster'] = kmeans[3].predict(data)\n",
    "df['principal_feature1'] = data[0]\n",
    "df['principal_feature2'] = data[1]\n",
    "df['cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28dfc5047004b769fd76d5cb2914805",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the different clusters with the 2 main principal components\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"cluster\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return Series of distance between each point and his distance with the closest centroid\n",
    "def getDistanceByPoint(data, model):\n",
    "    distance = []\n",
    "    for i in range(0,len(data)):\n",
    "        Xa = np.array(data.loc[i])\n",
    "        Xb = model.cluster_centers_[model.labels_[i]-1]\n",
    "        distance.append(np.linalg.norm(Xa-Xb))\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
    "distance = getDistanceByPoint(data, kmeans[3])\n",
    "number_of_outliers = int(outliers_fraction*len(distance))\n",
    "threshold = heapq.nlargest(number_of_outliers, distance)[-1]\n",
    "# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) \n",
    "df['anomaly21'] = (distance >= threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fce4dfc7157433f813b3ecf7ab611c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly with cluster view\n",
    "fig, ax = plt.subplots()\n",
    "colors = {0:'blue', 1:'red'}\n",
    "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly21\"].apply(lambda x: colors[x]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d0cc5ea4f5a49708b2e92da6e2026c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly21'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bf11bc5fe44435b8a36896191915d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly21'] == 0, 'value']\n",
    "b = df.loc[df['anomaly21'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories + Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation of 4 differents data set based on categories defined before\n",
    "df_class0 = df.loc[df['categories'] == 0, 'value']\n",
    "df_class1 = df.loc[df['categories'] == 1, 'value']\n",
    "df_class2 = df.loc[df['categories'] == 2, 'value']\n",
    "df_class3 = df.loc[df['categories'] == 3, 'value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e25bc8d01244e1ac57a6873b16996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f72490cca20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the temperature repartition by categories\n",
    "fig, axs = plt.subplots(2,2)\n",
    "df_class0.hist(ax=axs[0,0],bins=32)\n",
    "df_class1.hist(ax=axs[0,1],bins=32)\n",
    "df_class2.hist(ax=axs[1,0],bins=32)\n",
    "df_class3.hist(ax=axs[1,1],bins=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply ellipticEnvelope(gaussian distribution) at each categories\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class0.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class0 = pd.DataFrame(df_class0)\n",
    "df_class0['deviation'] = envelope.decision_function(X_train)\n",
    "df_class0['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class1.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class1 = pd.DataFrame(df_class1)\n",
    "df_class1['deviation'] = envelope.decision_function(X_train)\n",
    "df_class1['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class2.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class2 = pd.DataFrame(df_class2)\n",
    "df_class2['deviation'] = envelope.decision_function(X_train)\n",
    "df_class2['anomaly'] = envelope.predict(X_train)\n",
    "\n",
    "envelope =  EllipticEnvelope(contamination = outliers_fraction) \n",
    "X_train = df_class3.values.reshape(-1,1)\n",
    "envelope.fit(X_train)\n",
    "df_class3 = pd.DataFrame(df_class3)\n",
    "df_class3['deviation'] = envelope.decision_function(X_train)\n",
    "df_class3['anomaly'] = envelope.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bbdc11d91145788b4cb86cf8778390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the temperature repartition by categories with anomalies\n",
    "a0 = df_class0.loc[df_class0['anomaly'] == 1, 'value']\n",
    "b0 = df_class0.loc[df_class0['anomaly'] == -1, 'value']\n",
    "\n",
    "a1 = df_class1.loc[df_class1['anomaly'] == 1, 'value']\n",
    "b1 = df_class1.loc[df_class1['anomaly'] == -1, 'value']\n",
    "\n",
    "a2 = df_class2.loc[df_class2['anomaly'] == 1, 'value']\n",
    "b2 = df_class2.loc[df_class2['anomaly'] == -1, 'value']\n",
    "\n",
    "a3 = df_class3.loc[df_class3['anomaly'] == 1, 'value']\n",
    "b3 = df_class3.loc[df_class3['anomaly'] == -1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs[0,0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[0,1].hist([a1,b1], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[1,0].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[1,1].hist([a3,b3], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "axs[0,0].set_title(\"WeekEndNight\")\n",
    "axs[0,1].set_title(\"WeekEndLight\")\n",
    "axs[1,0].set_title(\"WeekDayNight\")\n",
    "axs[1,1].set_title(\"WeekDayLight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3651535df534f82aee459a0420e4602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_class = pd.concat([df_class0, df_class1, df_class2, df_class3])\n",
    "df['anomaly22'] = df_class['anomaly']\n",
    "df['anomaly22'] = np.array(df['anomaly22'] == -1).astype(int)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly22'] == 1, ('time_epoch', 'value')] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5283269a9834e8599ec110be5034335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly22'] == 0, 'value']\n",
    "b = df.loc[df['anomaly22'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7194\n",
      "1      73\n",
      "Name: anomaly25, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Take useful feature and standardize them \n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "data = pd.DataFrame(np_scaled)\n",
    "# train isolation forest \n",
    "model =  IsolationForest(contamination = outliers_fraction)\n",
    "model.fit(data)\n",
    "# add the data to the main  \n",
    "df['anomaly25'] = pd.Series(model.predict(data))\n",
    "df['anomaly25'] = df['anomaly25'].map( {1: 0, -1: 1} )\n",
    "print(df['anomaly25'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3a192126d44743824deaefbb0edb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly25'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c19c4bf1cf4574a5e7302c3ed8cab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = df.loc[df['anomaly25'] == 0, 'value']\n",
    "b = df.loc[df['anomaly25'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One class SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7198\n",
      "1      69\n",
      "Name: anomaly26, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "# Take useful feature and standardize them \n",
    "data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data)\n",
    "# train one class SVM \n",
    "model =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05\n",
    "data = pd.DataFrame(np_scaled)\n",
    "model.fit(data)\n",
    "# add the data to the main  \n",
    "df['anomaly26'] = pd.Series(model.predict(data))\n",
    "df['anomaly26'] = df['anomaly26'].map( {1: 0, -1: 1} )\n",
    "print(df['anomaly26'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2de97de76e47a2abda1adc0a3438df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly26'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2d13431db44a1aee79b0308a52c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = df.loc[df['anomaly26'] == 0, 'value']\n",
    "b = df.loc[df['anomaly26'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_n = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]\n",
    "min_max_scaler = preprocessing.StandardScaler()\n",
    "np_scaled = min_max_scaler.fit_transform(data_n)\n",
    "data_n = pd.DataFrame(np_scaled)\n",
    "\n",
    "# important parameters and train/test size\n",
    "prediction_time = 1 \n",
    "testdatasize = 1000\n",
    "unroll_length = 50\n",
    "testdatacut = testdatasize + unroll_length  + 1\n",
    "\n",
    "#train data\n",
    "x_train = data_n[0:-prediction_time-testdatacut].to_numpy()\n",
    "y_train = data_n[prediction_time:-testdatacut  ][0].to_numpy()\n",
    "\n",
    "# test data\n",
    "x_test = data_n[0-testdatacut:-prediction_time].to_numpy()\n",
    "y_test = data_n[prediction_time-testdatacut:  ][0].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (6165, 50, 5)\n",
      "y_train (6165,)\n",
      "x_test (1000, 50, 5)\n",
      "y_test (1000,)\n"
     ]
    }
   ],
   "source": [
    "#unroll: create sequence of 50 previous data points for each data points\n",
    "def unroll(data,sequence_length=24):\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    return np.asarray(result)\n",
    "\n",
    "# adapt the datasets for the sequence data shape\n",
    "x_train = unroll(x_train,unroll_length)\n",
    "x_test  = unroll(x_test,unroll_length)\n",
    "y_train = y_train[-x_train.shape[0]:]\n",
    "y_test  = y_test[-x_test.shape[0]:]\n",
    "\n",
    "# see the shape\n",
    "print(\"x_train\", x_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# specific libraries for RNN\n",
    "# keras is a high layer build on Tensorflow layer to stay in high level/easy implementation\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "import time #helper libraries\n",
    "from keras.models import model_from_json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  import sys\n",
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(return_sequences=True, input_shape=(None, 5), units=50)`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compilation time : 0.015527009963989258\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(\n",
    "    input_dim=x_train.shape[-1],\n",
    "    output_dim=50,\n",
    "    return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(\n",
    "    100,\n",
    "    return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(\n",
    "    units=1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "start = time.time()\n",
    "model.compile(loss='mse', optimizer='rmsprop')\n",
    "print('compilation time : {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5548 samples, validate on 617 samples\n",
      "Epoch 1/30\n",
      "5548/5548 [==============================] - 5s 914us/step - loss: 0.6115 - val_loss: 0.7325\n",
      "Epoch 2/30\n",
      "5548/5548 [==============================] - 3s 521us/step - loss: 0.2077 - val_loss: 0.2981\n",
      "Epoch 3/30\n",
      "5548/5548 [==============================] - 3s 614us/step - loss: 0.2268 - val_loss: 0.2701\n",
      "Epoch 4/30\n",
      "5548/5548 [==============================] - 3s 504us/step - loss: 0.1122 - val_loss: 0.2326\n",
      "Epoch 5/30\n",
      "5548/5548 [==============================] - 3s 463us/step - loss: 0.1041 - val_loss: 0.2232\n",
      "Epoch 6/30\n",
      "5548/5548 [==============================] - 3s 452us/step - loss: 0.1101 - val_loss: 0.2194\n",
      "Epoch 7/30\n",
      "5548/5548 [==============================] - 3s 520us/step - loss: 0.1192 - val_loss: 0.1829\n",
      "Epoch 8/30\n",
      "5548/5548 [==============================] - 3s 471us/step - loss: 0.0907 - val_loss: 0.1701\n",
      "Epoch 9/30\n",
      "5548/5548 [==============================] - 3s 487us/step - loss: 0.0853 - val_loss: 0.1635\n",
      "Epoch 10/30\n",
      "5548/5548 [==============================] - 3s 535us/step - loss: 0.0855 - val_loss: 0.1584\n",
      "Epoch 11/30\n",
      "5548/5548 [==============================] - 3s 535us/step - loss: 0.0896 - val_loss: 0.1630\n",
      "Epoch 12/30\n",
      "5548/5548 [==============================] - 3s 490us/step - loss: 0.0968 - val_loss: 0.1676\n",
      "Epoch 13/30\n",
      "5548/5548 [==============================] - 3s 522us/step - loss: 0.0893 - val_loss: 0.1778\n",
      "Epoch 14/30\n",
      "5548/5548 [==============================] - 3s 521us/step - loss: 0.0918 - val_loss: 0.1946\n",
      "Epoch 15/30\n",
      "5548/5548 [==============================] - 3s 451us/step - loss: 0.0913 - val_loss: 0.1478\n",
      "Epoch 16/30\n",
      "5548/5548 [==============================] - 2s 440us/step - loss: 0.0784 - val_loss: 0.1372\n",
      "Epoch 17/30\n",
      "5548/5548 [==============================] - 3s 524us/step - loss: 0.0735 - val_loss: 0.1325\n",
      "Epoch 18/30\n",
      "5548/5548 [==============================] - 3s 484us/step - loss: 0.0722 - val_loss: 0.1420\n",
      "Epoch 19/30\n",
      "5548/5548 [==============================] - 3s 529us/step - loss: 0.0741 - val_loss: 0.1473\n",
      "Epoch 20/30\n",
      "5548/5548 [==============================] - 2s 431us/step - loss: 0.0798 - val_loss: 0.1184\n",
      "Epoch 21/30\n",
      "5548/5548 [==============================] - 2s 435us/step - loss: 0.0795 - val_loss: 0.1216\n",
      "Epoch 22/30\n",
      "5548/5548 [==============================] - 3s 468us/step - loss: 0.0699 - val_loss: 0.1222\n",
      "Epoch 23/30\n",
      "5548/5548 [==============================] - 3s 481us/step - loss: 0.0708 - val_loss: 0.1136\n",
      "Epoch 24/30\n",
      "5548/5548 [==============================] - 3s 511us/step - loss: 0.0735 - val_loss: 0.1090\n",
      "Epoch 25/30\n",
      "5548/5548 [==============================] - 3s 523us/step - loss: 0.0741 - val_loss: 0.1213\n",
      "Epoch 26/30\n",
      "5548/5548 [==============================] - 3s 492us/step - loss: 0.0695 - val_loss: 0.1046\n",
      "Epoch 27/30\n",
      "5548/5548 [==============================] - 3s 545us/step - loss: 0.0680 - val_loss: 0.1122\n",
      "Epoch 28/30\n",
      "5548/5548 [==============================] - 3s 535us/step - loss: 0.0872 - val_loss: 0.1096\n",
      "Epoch 29/30\n",
      "5548/5548 [==============================] - 3s 565us/step - loss: 0.0709 - val_loss: 0.1047\n",
      "Epoch 30/30\n",
      "5548/5548 [==============================] - 3s 475us/step - loss: 0.0684 - val_loss: 0.1033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f720c121ef0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=3028,\n",
    "    nb_epoch=30,\n",
    "    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = model\n",
    "diff=[]\n",
    "ratio=[]\n",
    "p = loaded_model.predict(x_test)\n",
    "# predictions = lstm.predict_sequences_multiple(loaded_model, x_test, 50, 50)\n",
    "for u in range(len(y_test)):\n",
    "    pr = p[u][0]\n",
    "    ratio.append((y_test[u]/pr)-1)\n",
    "    diff.append(abs(y_test[u]- pr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35cb699c6dd4de1ae618e9db1199aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots()\n",
    "axs.plot(p,color='red', label='prediction')\n",
    "axs.plot(y_test,color='blue', label='y_test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    7257\n",
      "1      10\n",
      "Name: anomaly27, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# select the most distant prediction/reality data points as anomalies\n",
    "diff = pd.Series(diff)\n",
    "number_of_outliers = int(outliers_fraction*len(diff))\n",
    "threshold = diff.nlargest(number_of_outliers).min()\n",
    "# data with anomaly label (test data part)\n",
    "test = (diff >= threshold).astype(int)\n",
    "# the training data part where we didn't predict anything (overfitting possible): no anomaly\n",
    "complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))\n",
    "# # add the data to the main\n",
    "df['anomaly27'] = complement.append(test, ignore_index='True')\n",
    "print(df['anomaly27'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb75dd015944354bd6e4a9e8c43c422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualisation of anomaly throughout time (viz 1)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "a = df.loc[df['anomaly27'] == 1, ['time_epoch', 'value']] #anomaly\n",
    "\n",
    "ax.plot(df['time_epoch'], df['value'], color='blue')\n",
    "ax.scatter(a['time_epoch'],a['value'], color='red')\n",
    "plt.axis([1.370*1e7, 1.405*1e7, 15,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b144a407a30d460da49cb1ce3e55604a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    }
   ],
   "source": [
    "# visualisation of anomaly with temperature repartition (viz 2)\n",
    "a = df.loc[df['anomaly27'] == 0, 'value']\n",
    "b = df.loc[df['anomaly27'] == 1, 'value']\n",
    "\n",
    "fig, axs = plt.subplots()\n",
    "axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "rise": {
   "center": false,
   "transition": "fade"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
